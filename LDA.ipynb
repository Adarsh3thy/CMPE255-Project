{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xf5CtmIMz1_c",
        "outputId": "c2342961-039a-4a8b-e959-eef3b35055d1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1_0-FQlz91X",
        "outputId": "2b0b4d65-5b97-4e78-da8e-8e6d7ff8dde8"
      },
      "source": [
        "!ls /content/drive/MyDrive/C50-data/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data  data.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neYRC3BN0FXN"
      },
      "source": [
        "\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob, csv\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import seaborn as sns\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus.reader.wordnet import VERB, NOUN, ADJ, ADV\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import chi2,SelectKBest\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import model_selection\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qap7ykOK0IlK"
      },
      "source": [
        "def dataframe_creation(path):\n",
        "    labels = []\n",
        "    docs = []\n",
        "    #path='./data/C50train/'\n",
        "    for r, dirs, files in os.walk(path):\n",
        "         for file in files:\n",
        "               # print(file)\n",
        "                with open(os.path.join(r, file), \"r\") as f:\n",
        "                    author_text = f.read()\n",
        "                    author_sentences = author_text.replace('.\\n','')\n",
        "                    docs.append(author_sentences)\n",
        "                    labels.append(r.replace(path,''))\n",
        "\n",
        "                    #author_sentences = author_text.split(\".\\n\")\n",
        "                    #for sentence in author_sentences:\n",
        "                    #        sentence = sentence.replace('\\n', '')\n",
        "                    #        if sentence.__len__() > 0 and sentence.count(' ') > 1:\n",
        "                    #            docs.append(sentence)\n",
        "                    #            labels.append(r.replace(path,''))\n",
        "                    \n",
        "    data_dict = dict([('text', docs), ('author', labels)])\n",
        "    df = pd.DataFrame(data_dict)\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    df['author_label'] = le.fit_transform(df.author)\n",
        "    df['id'] = df.index\n",
        "    \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9PWtOTw0LqE"
      },
      "source": [
        "df_trainset = pd.read_csv('/content/drive/MyDrive/preprocessed_trainset.csv')\n",
        "df_testset= pd.read_csv('/content/drive/MyDrive/preprocessed_testset.csv')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        },
        "id": "EcYmf9sUnuzc",
        "outputId": "385ff47d-d565-49e5-eea8-6343b8131b08"
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 22.6 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 29.8 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 81 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 317 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 327 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 337 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 348 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 358 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 368 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 378 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 389 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 399 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 409 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 419 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 430 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 440 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 450 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 460 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 471 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 481 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 491 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 501 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 512 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 522 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 532 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 542 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 552 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 563 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 573 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 583 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 593 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 604 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 614 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 624 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 634 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 645 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 655 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 665 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 675 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 686 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 696 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 706 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 716 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 727 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 737 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 747 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 757 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 768 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 778 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 788 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 798 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 808 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 819 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 829 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 839 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 849 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 860 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 870 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 880 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 890 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 901 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 911 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 921 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 931 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 942 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 952 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 962 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 972 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 983 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 993 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.5 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.5 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.5 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.5 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.5 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.5 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.5 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.5 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.5 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.6 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.6 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.6 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.6 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.6 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.6 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.6 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7 MB 5.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.1.0)\n",
            "Collecting pandas>=1.2.0\n",
            "  Downloading pandas-1.3.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 31.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.1)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.16-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.7.3)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n",
            "Collecting numpy>=1.20.0\n",
            "  Downloading numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 47.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.4.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyLDAvis) (3.0.0)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136897 sha256=094cee78f6d98bdefce3773773453c5e57b07df7426137b9cc754c83669218c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: numpy, pandas, funcy, pyLDAvis\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.4 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.4 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed funcy-1.16 numpy-1.21.4 pandas-1.3.4 pyLDAvis-3.3.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NZWBOs80N9t",
        "outputId": "ca6bb442-c784-4b8d-d529-84f91a8b675a"
      },
      "source": [
        "import pickle\n",
        "import gensim\n",
        "import pyLDAvis\n",
        "import gensim\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import nltk; nltk.download('stopwords')\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "import re\n",
        "import warnings\n",
        "from pprint import pprint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import seaborn as sns\n",
        "%config InlineBackend.figure_formats = ['retina']\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn import linear_model\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import fbeta_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6slxfuwc1Kek"
      },
      "source": [
        "df_trainset['preprocessed_text_len'] = df_trainset['preprocessed_text'].apply(lambda x: len(x.split()))\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRDQPzrB3uds"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['come','order','try','go','get','make','drink','plate','dish','restaurant','place',\n",
        "                  'would','really','like','great','service','came','got','amp'])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9Jevw2A2SHW",
        "outputId": "4f78794d-41e6-4806-b666-063e56b7ddf6"
      },
      "source": [
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "def strip_newline(series):\n",
        "    return [review.replace('\\n','') for review in series]\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "        \n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def bigrams(words, bi_min=15, tri_min=10):\n",
        "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
        "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "    return bigram_mod"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/catalogue.py:138: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
            "  for entry_point in AVAILABLE_ENTRY_POINTS.get(self.entry_point_namespace, []):\n",
            "/usr/local/lib/python3.7/dist-packages/catalogue.py:138: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
            "  for entry_point in AVAILABLE_ENTRY_POINTS.get(self.entry_point_namespace, []):\n",
            "/usr/local/lib/python3.7/dist-packages/catalogue.py:126: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
            "  for entry_point in AVAILABLE_ENTRY_POINTS.get(self.entry_point_namespace, []):\n",
            "/usr/local/lib/python3.7/dist-packages/catalogue.py:138: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
            "  for entry_point in AVAILABLE_ENTRY_POINTS.get(self.entry_point_namespace, []):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XZmGHPe2w1b"
      },
      "source": [
        "def get_corpus(df):\n",
        "    \"\"\"\n",
        "    Get Bigram Model, Corpus, id2word mapping\n",
        "    \"\"\"\n",
        "    \n",
        "    df['preprocessed_text'] = strip_newline(df.preprocessed_text)\n",
        "    words = (sent_to_words(df.preprocessed_text))\n",
        "    words = remove_stopwords(words)\n",
        "    bigram = bigrams(words)\n",
        "    bigram = [bigram[review] for review in words]\n",
        "#     lemma = lemmatization(bigram)\n",
        "    id2word = gensim.corpora.Dictionary(bigram)\n",
        "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
        "    id2word.compactify()\n",
        "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
        "    return corpus, id2word, bigram"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3fW3-DK3Nww"
      },
      "source": [
        "train_corpus4, train_id2word4, bigram_train4 = get_corpus(df_trainset)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40MuLXH_32Rr"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter('ignore')\n",
        "    lda_train4 = gensim.models.ldamulticore.LdaMulticore(\n",
        "                           corpus=train_corpus4,\n",
        "                           num_topics=50,\n",
        "                           id2word=train_id2word4,\n",
        "                           chunksize=100,\n",
        "                           workers=7, # Num. Processing Cores - 1\n",
        "                           passes=50,\n",
        "                           eval_every = 1,\n",
        "                           per_word_topics=True)\n",
        "    lda_train4.save('/content/drive/MyDrive/lda_train4.model')"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2-gHlOnpycB"
      },
      "source": [
        "topn_words = {'Topic_' + str(i): [word for word, prob in lda_train4.show_topic(i, topn=50)] for i in range(0, lda_train4.num_topics)}"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btTbP6MRqVvD"
      },
      "source": [
        "top_words=topn_words.values()"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h66vzn2qyLL"
      },
      "source": [
        "top_words_flat=[]\n",
        "for list in top_words:\n",
        "  for word in list:\n",
        "    top_words_flat.append(word.replace('_',' '))"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKT6RDDVp70v",
        "outputId": "cef98faf-bb88-4a9f-8666-b2605cafcee7"
      },
      "source": [
        "top_words_flat[:10]"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['building',\n",
              " 'police',\n",
              " 'day',\n",
              " 'away',\n",
              " 'protest',\n",
              " 'office',\n",
              " 'city',\n",
              " 'outside',\n",
              " 'ontario',\n",
              " 'employee']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tHY4IIK4nI8"
      },
      "source": [
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/toptopics.csv', 'w', newline='') as csv_1:\n",
        "  csv_out = csv.writer(csv_1)\n",
        "  csv_out.writerows([top_words_flat[index]] for index in range(0, len(top_words_flat)))\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmlAXtI46YYW"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}