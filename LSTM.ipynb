{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK4P0iVHcZwX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMQH-MoTd1Hp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1501f99a-98a6-4422-a1ae-76b68fbf5452"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3JH7u4XgacJ"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7S3AWW3sd1pq",
        "outputId": "abc97d00-e415-4f14-ee06-9d9bb539f5d8"
      },
      "source": [
        "!ls /content/drive/MyDrive/CMPE-255/data/C50train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AaronPressman\t  JaneMacartney      LydiaZajc\t      RobinSidel\n",
            "AlanCrosby\t  JanLopatka\t     LynneO_Donnell   RogerFillion\n",
            "AlexanderSmith\t  JimGilchrist\t     LynnleyBrowning  SamuelPerry\n",
            "BenjaminKangLim   JoeOrtiz\t     MarcelMichelson  ScottHillis\n",
            "BernardHickey\t  JohnMastrini\t     MarkBendeich     SimonCowell\n",
            "BradDorfman\t  JonathanBirt\t     MartinWolk       TanEeLyn\n",
            "DarrenSchuettler  JoWinterbottom     MatthewBunce     TheresePoletti\n",
            "DavidLawder\t  KarlPenhaul\t     MichaelConnor    TimFarrand\n",
            "EdnaFernandes\t  KeithWeir\t     MureDickie       ToddNissen\n",
            "EricAuchard\t  KevinDrawbaugh     NickLouth\t      WilliamKazer\n",
            "FumikoFujisaki\t  KevinMorrison      PatriciaCommins\n",
            "GrahamEarnshaw\t  KirstinRidley      PeterHumphrey\n",
            "HeatherScoffield  KouroshKarimkhany  PierreTran\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx_XE_U8d1tF"
      },
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob, csv\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import seaborn as sns\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus.reader.wordnet import VERB, NOUN, ADJ, ADV\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import chi2,SelectKBest\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import model_selection\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbNI5qCgj5l6"
      },
      "source": [
        "def dataframe_creation(path):\n",
        "    labels = []\n",
        "    docs = []\n",
        "    #path='./data/C50train/'\n",
        "    for r, dirs, files in os.walk(path):\n",
        "         for file in files:\n",
        "               # print(file)\n",
        "                with open(os.path.join(r, file), \"r\") as f:\n",
        "                    author_text = f.read()\n",
        "                    author_sentences = author_text.replace('.\\n','')\n",
        "                    docs.append(author_sentences)\n",
        "                    labels.append(r.replace(path,''))\n",
        "\n",
        "                    #author_sentences = author_text.split(\".\\n\")\n",
        "                    #for sentence in author_sentences:\n",
        "                    #        sentence = sentence.replace('\\n', '')\n",
        "                    #        if sentence.__len__() > 0 and sentence.count(' ') > 1:\n",
        "                    #            docs.append(sentence)\n",
        "                    #            labels.append(r.replace(path,''))\n",
        "                    \n",
        "    data_dict = dict([('text', docs), ('author', labels)])\n",
        "    df = pd.DataFrame(data_dict)\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    df['author_label'] = le.fit_transform(df.author)\n",
        "    df['id'] = df.index\n",
        "    \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGhDXk7hj5wN"
      },
      "source": [
        "df_trainset=dataframe_creation('/content/drive/MyDrive/CMPE-255/data/C50train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "R2aZk4dej5y5",
        "outputId": "945f08ef-f044-4b21-9145-8af446bed3b1"
      },
      "source": [
        "df_trainset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "      <th>author_label</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Britain's Guardian Royal Exchange said on Tues...</td>\n",
              "      <td>/SimonCowell</td>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Royal &amp;amp; Sun Alliance Group plc , one of Br...</td>\n",
              "      <td>/SimonCowell</td>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Prudential Corp Plc, Britain's biggest life in...</td>\n",
              "      <td>/SimonCowell</td>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Almost 8 out of 10 people in Britain are oppos...</td>\n",
              "      <td>/SimonCowell</td>\n",
              "      <td>43</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Britain's largest insurance group Prudential C...</td>\n",
              "      <td>/SimonCowell</td>\n",
              "      <td>43</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ... id\n",
              "0  Britain's Guardian Royal Exchange said on Tues...  ...  0\n",
              "1  Royal &amp; Sun Alliance Group plc , one of Br...  ...  1\n",
              "2  Prudential Corp Plc, Britain's biggest life in...  ...  2\n",
              "3  Almost 8 out of 10 people in Britain are oppos...  ...  3\n",
              "4  Britain's largest insurance group Prudential C...  ...  4\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCQuOvMFj52W"
      },
      "source": [
        "df_test=dataframe_creation('/content/drive/MyDrive/CMPE-255/data/C50test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "E7qwIWGekcJe",
        "outputId": "73d49f84-7217-4dd6-c0d3-e3906bf357d6"
      },
      "source": [
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "      <th>author_label</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>U.S. Vice President Al Gore arrived in Beijing...</td>\n",
              "      <td>/WilliamKazer</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>China's offer to return a hijacker to Taiwan h...</td>\n",
              "      <td>/WilliamKazer</td>\n",
              "      <td>49</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>China called on Japan on Thursday to stop enco...</td>\n",
              "      <td>/WilliamKazer</td>\n",
              "      <td>49</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Citizens spilled into public places across Chi...</td>\n",
              "      <td>/WilliamKazer</td>\n",
              "      <td>49</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In China's headlong rush to develop, first in ...</td>\n",
              "      <td>/WilliamKazer</td>\n",
              "      <td>49</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ... id\n",
              "0  U.S. Vice President Al Gore arrived in Beijing...  ...  0\n",
              "1  China's offer to return a hijacker to Taiwan h...  ...  1\n",
              "2  China called on Japan on Thursday to stop enco...  ...  2\n",
              "3  Citizens spilled into public places across Chi...  ...  3\n",
              "4  In China's headlong rush to develop, first in ...  ...  4\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNAR5SqakcPz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50e84b68-39fa-4a13-df97-18879ce2e856"
      },
      "source": [
        "!pip install wordcloud\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from wordcloud) (1.19.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud) (7.1.2)\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ET_c-lbkcSE"
      },
      "source": [
        "eng_stopwords = set(stopwords.words(\"english\"))\n",
        "stop_words = stopwords.words('english')\n",
        "eng_stop_words=list(ENGLISH_STOP_WORDS)\n",
        "my_stop_words=['fax']\n",
        "eng_stop_words.extend(my_stop_words)\n",
        "stopwords_dict = Counter(stop_words)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "    \n",
        "def stop_word_rem_and_lemmatization(text):\n",
        "    lemm_text=[]\n",
        "    #stop_words_cleaned_text=[word for word in text.split() if word not in stopwords_dict]\n",
        "    \n",
        "    \n",
        "    for word,tag in pos_tag(text.split()):\n",
        "        lemm_text.append(lemmatizer.lemmatize(word, get_wordnet_pos(tag)))\n",
        "    #lemm_text=[word for word in lemm_text if word not in eng_stop_words]\n",
        "    return \" \".join(lemm_text).lower()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOobKd-kkcUv"
      },
      "source": [
        "remove_digits = str.maketrans('', '', digits)\n",
        "\n",
        "def preprocessData(text):\n",
        "    #Text lemmatization and stop words removal\n",
        "    sentence=stop_word_rem_and_lemmatization(text)\n",
        "    #print(sentence)\n",
        "    \n",
        "    \n",
        "    #remove dots between abbreviations, U.S.A--> USA\n",
        "    sentence=re.sub('\\.(?!(\\S[^. ])|\\d)', '', sentence)\n",
        "    \n",
        "    #pad punctuations with spaces\n",
        "    sentence = re.sub('([.,!;\"?\\'~#()&/{}_<>:|\\-\\*$])', r' \\1 ', sentence)\n",
        "         \n",
        "    #remove all characters less than or equal to two characters\n",
        "    #sentence=re.sub(r'\\b\\w{1,2}\\b', '', sentence)\n",
        "    \n",
        "    #remove all digits\n",
        "    #sentence = ''.join([i for i in sentence if not i.isdigit()])\n",
        "    sentence = sentence.translate(remove_digits)\n",
        "    \n",
        "    for ch in punctuation+'*()[]^@+=_/\\$%-.,':\n",
        "        sentence = sentence.replace(ch,'')\n",
        "    \n",
        "    #remove multiple whitespaces\n",
        "    sentence= re.sub(' +', ' ', sentence)\n",
        "    \n",
        "    #sentence=re.sub('[^A-Za-z0-9\\s]+', '', sentence)\n",
        "            \n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-57Pm8CkcXI"
      },
      "source": [
        "df_trainset[\"preprocessed_text\"] = df_trainset[\"text\"].apply(lambda x: preprocessData(str(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxHxe5A7kcae"
      },
      "source": [
        "df_test[\"preprocessed_text\"] = df_test[\"text\"].apply(lambda x: preprocessData(str(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mv2rB4Hrtaq"
      },
      "source": [
        "y_train = df_trainset['author_label']\n",
        "y_test = df_test['author_label']\n",
        "X_train = df_trainset[\"preprocessed_text\"]\n",
        "X_test = df_test[\"preprocessed_text\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg5izEuBrtdK"
      },
      "source": [
        "# import torch\n",
        "import torchtext.legacy as torchtext\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "TEXT = data.Field(sequential=True, tokenize=\"spacy\", lower=True, include_lengths=True)\n",
        "LABEL = data.Field(sequential=False)\n",
        "train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "TEXT.build_vocab(train, vectors=GloVe(name=\"6B\", dim=300),\n",
        "                 max_size=10000, min_freq=10)\n",
        "\n",
        "\n",
        "SCORE = data.Field(sequential=False, use_vocab=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEpLyGL3rtfx"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import pandas as pd\n",
        "import dill\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90yJNi2prtiH"
      },
      "source": [
        "class AuthorClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, mode, output_size, hidden_size, vocab_size, embedding_length, word_embeddings):\n",
        "      super(AuthorClassifier, self).__init__()\n",
        "\n",
        "      if mode not in ['rnn', 'lstm', 'gru', 'bilstm']:\n",
        "        raise ValueError(\"Choose a mode from - rnn / lstm / gru / bilstm\")\n",
        "\n",
        "      self.mode = mode\n",
        "      self.output_size = output_size\n",
        "      self.hidden_size = hidden_size\n",
        "      self.vocab_size = vocab_size\n",
        "      self.embedding_length = embedding_length\n",
        "      self.embedding = nn.Embedding(self.vocab_size,self.embedding_length)\n",
        "      self.embedding.weight = nn.Parameter(word_embeddings,requires_grad = False)\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "       \n",
        "      if self.mode == 'rnn':\n",
        "        self.network = nn.RNN(self.embedding_length,self.hidden_size)\n",
        "      elif self.mode == 'lstm':\n",
        "        self.network = nn.LSTM(self.embedding_length,self.hidden_size)\n",
        "      elif self.mode == 'gru':\n",
        "        self.network = nn.GRU(self.embedding_length,self.hidden_size)\n",
        "      elif self.mode == 'bilstm':\n",
        "        self.network = nn.LSTM(self.embedding_length,self.hidden_size,bidirectional = True)\n",
        "\n",
        "     \n",
        "      self.fclayer = nn.Linear(self.hidden_size,self.output_size)\n",
        "      \n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "      text_embeddings = self.embedding(text)\n",
        "      pack_sequence = nn.utils.rnn.pack_padded_sequence(text_embeddings,text_lengths)\n",
        "\n",
        "      if self.mode in ('lstm','bilstm'):\n",
        "        a,(hidden,cell) = self.network(pack_sequence)\n",
        "        if self.mode == 'bilstm':\n",
        "          hidden = hidden[0,:,:]+ hidden[1,:,:]\n",
        "      else:\n",
        "        a,hidden = self.network(pack_sequence) \n",
        "      hidden = hidden.squeeze(0)\n",
        "      pred = self.fclayer(hidden)\n",
        "      return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nhdt8R4yrtlf"
      },
      "source": [
        "import sklearn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "def train_classifier(model, dataset_iterator, loss_function, optimizer, num_epochs, log = \"runs\", verbose = False, recurrent = True):\n",
        "  writer = SummaryWriter(log_dir=log)\n",
        "  model.train()\n",
        "  step = 0\n",
        "  f1score_train = []\n",
        "  accuracy_train = []\n",
        "  loss_train = []\n",
        "  for epoch in range(num_epochs):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0\n",
        "    f1 = 0\n",
        "    f1_step = 0\n",
        "    \n",
        "    for batch in dataset_iterator:\n",
        "      comment, comment_lengths = batch.text\n",
        "      labels = batch.Author_num\n",
        "\n",
        "      batch_size = len(labels)\n",
        "      optimizer.zero_grad()\n",
        "      output = model(comment, comment_lengths).squeeze(0)\n",
        "\n",
        "      loss = loss_function(output, labels.long())\n",
        "      loss.backward() \n",
        "      nn.utils.clip_grad_norm(model.parameters(),0.5)\n",
        "      optimizer.step()\n",
        "\n",
        "      pred = torch.max(output.data,1).indices\n",
        "      f1 += sklearn.metrics.f1_score((labels.cpu()).numpy(), (pred.cpu()).numpy(),average= 'macro')\n",
        "      correct += (torch.sum(pred == labels)).item()\n",
        "      total += len(labels)\n",
        "      total_loss += loss.item()\n",
        "      f1_step += 1\n",
        "\n",
        "      if ((step % 100) == 0):\n",
        "        writer.add_scalar(\"Loss/train\", total_loss/total, step)\n",
        "        writer.add_scalar(\"Acc/train\", correct/total, step)\n",
        "        writer.add_scalar(\"F1 Score/train\", f1/f1_step, step)\n",
        "        \n",
        "      step = step+1\n",
        "    f1score_train.append(f1/f1_step)\n",
        "    loss_train.append(total_loss/total)\n",
        "    accuracy_train.append(correct/total)\n",
        "    print('---Training statistics---',\"Epoch: %s Acc: %s Loss: %s\"%(epoch+1, correct/total, total_loss/total),'F1 Score:',f1/f1_step,)\n",
        "\n",
        "  return loss_train,f1score_train,accuracy_train\n",
        "\n",
        "def evaluate_classifier(model, dataset_iterator, loss_function, recurrent = True):\n",
        "  model.eval()\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  total_loss = 0\n",
        "  overall_pred = []\n",
        "  overall_label = []\n",
        "  accuracy_test = []\n",
        "  loss_test = []\n",
        "  f1_step = 0\n",
        "  f1 = 0\n",
        "\n",
        "  for batch in dataset_iterator:\n",
        "    comment, comment_lengths = batch.text\n",
        "    labels = batch.Author_num\n",
        "    output = model(comment, comment_lengths).squeeze(0)\n",
        "    loss = loss_function(output, labels.long())\n",
        "    pred = torch.max(output.data,1).indices \n",
        "    correct += (torch.sum(pred == labels)).item()\n",
        "    total += len(labels)\n",
        "    total_loss += loss.item()\n",
        "    ap = pred.cpu()\n",
        "    a = np.asarray(ap)\n",
        "    labels = labels.cpu()\n",
        "    b = np.asarray(labels)\n",
        "    f1_step += 1\n",
        "    overall_pred.append(a)\n",
        "    overall_label.append(b)\n",
        "\n",
        "  overall_p= [val for sublist in overall_pred for val in sublist]\n",
        "  overall_l = [val for sublist in overall_label for val in sublist]\n",
        "  f1ss = sklearn.metrics.f1_score(overall_l,overall_p,average= 'macro')\n",
        "  accuracy_test.append(correct/total)\n",
        "  loss_test.append(total_loss/total)\n",
        "  print(\"Validation statistics: Acc: %s Loss: %s\"%(correct/total, total_loss/total),'F1 Score:',f1ss)\n",
        "  return overall_pred,overall_label,accuracy_test,f1ss,loss_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ0S9BXWwymJ"
      },
      "source": [
        "device = torch.device('cuda:0')\n",
        "BATCH_SIZE = 64\n",
        "train_iterator = data.BucketIterator(\n",
        "    train, \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch = True,\n",
        "    repeat=False, \n",
        "    shuffle=True,\n",
        "    device = device)\n",
        "\n",
        "#LSTM \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torchtext.legacy import vocab\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "plt.figure(figsize = (10,10))\n",
        "\n",
        "\n",
        "output_size = 50\n",
        "hidden_size = 300\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_length = 100\n",
        "word_embeddings = TEXT.vocab.vectors\n",
        "num_epochs = 1\n",
        "mode = 'lstm'\n",
        "\n",
        "model = AuthorClassifier(mode, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "log_dir = 'runs/lstm1'\n",
        "final_acc_train_lstm  = []\n",
        "final_loss_train_lstm = []\n",
        "final_loss_test_lstm = []\n",
        "final_acc_test_lstm = []\n",
        "final_f1score_train_lstm = []\n",
        "final_f1score_test_lstm = []\n",
        "\n",
        "\n",
        "for multi in range(20):\n",
        "  loss_train,f1score,accs = train_classifier(model, train_iterator, loss_function, optimizer, log = log_dir, num_epochs = num_epochs)\n",
        "  overall_pred,overall_label,accs_test,f1ss,loss_test = evaluate_classifier(model, val_iterator, loss_function)\n",
        "  final_acc_train_lstm.append(accs[0])\n",
        "  final_acc_test_lstm.append(accs_test[0])\n",
        "  final_f1score_train_lstm.append(f1score[0])\n",
        "  final_f1score_test_lstm.append(f1ss)\n",
        "  final_loss_train_lstm.append(loss_train[0])\n",
        "  final_loss_test_lstm.append(loss_test[0])\n",
        "\n",
        "cf = np.zeros((50,50))\n",
        "\n",
        "overall_pred = [val for sublist in overall_pred for val in sublist]\n",
        "\n",
        "overall_label = [val for sublist in overall_label for val in sublist]\n",
        "\n",
        "ziplist = list(zip(overall_label,overall_pred))\n",
        "for coordinate in ziplist:\n",
        "  cf[coordinate]+=1\n",
        "ax = sns.heatmap(cf,annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMWQh_-Bwyoc"
      },
      "source": [
        "#Plot - accuracy\n",
        "import plotly.graph_objects as go\n",
        "fig_accuracy = go.Figure()\n",
        "\n",
        "fig_accuracy.add_trace(go.Scatter(\n",
        "    y=final_acc_train_lstm,\n",
        "    connectgaps=True, marker_color='rgba(128, 0, 0, 0.9)', name = 'Training accuracy lstm'))\n",
        "\n",
        "fig_accuracy.add_trace(go.Scatter(\n",
        "    y=final_acc_test_lstm,\n",
        "    connectgaps=True, marker_color='rgba(255, 0, 0, 0.9)', name = 'Testing accuracy lstm'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyvvbvgowysW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}